{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Nadja_Part2_LogisticRegression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4XUpNjzbmqg"
      },
      "source": [
        "# Logistic Regression by Nadja Fejzic\n",
        "\n",
        "February 2021\n",
        "\n",
        "In the assignment, I used gradient ascent to find the weights for the logistic regression using the widely-used breast cancer data set described here:\n",
        "\n",
        "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpH4iQMtbmqo"
      },
      "source": [
        "## Step 1:  Getting, preprocessing, and understanding the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z4mPwV7bmqp"
      },
      "source": [
        "### Importing the standard libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unvG0w4wbmqq"
      },
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "# Importing breast cancer dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "# Importing preprocessing from sklearn\n",
        "from sklearn import preprocessing\n",
        "# Importing train_test_split from sklearn to be able to split the sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCxARRbSbmqq"
      },
      "source": [
        "### Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Evc1dkaqbmqr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6d3ee4e-47ac-4fc5-cc69-b39238f8c49d"
      },
      "source": [
        "# Loading dataset to a variable cancer\n",
        "cancer = load_breast_cancer()\n",
        "print(\"This is the example of logistic regression using breast cancer dataset\\n\")\n",
        "# Storing target to a variable called y\n",
        "y = cancer.target\n",
        "# Store feature to a variable called X\n",
        "X = cancer.data\n",
        "\n",
        "# Scaling\n",
        "scaler = preprocessing.StandardScaler().fit(X)\n",
        "X = scaler.transform(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is the example of logistic regression using breast cancer dataset\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1ZVvr6Tbmqr",
        "outputId": "4290f56a-72c9-4c18-e762-76f03068eed7"
      },
      "source": [
        "# Printing the shape of data (X) and target (Y) values \n",
        "print(\"The shape of X --\", X.shape)\n",
        "print(\"The shape of Y --\", y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of X -- (569, 30)\n",
            "The shape of Y -- (569,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cN2HmpJbmqs"
      },
      "source": [
        "### Data Pre-Processing\n",
        "#### Splitting the data into train and test before scaling the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_gLT2gMbmqs"
      },
      "source": [
        "# Using train_test_split() function to split the dataset - controls the shuffling applied to the data before applying the split and makes the split always the same \n",
        "# By specifying random state we make sure that split is fixed (eg. Data split is the same in every run)\n",
        "# Storing the returned value to X_train, X_test, y_train, y_test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 21) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsllRvb-bmqt"
      },
      "source": [
        "#### Scale the data since we will be using gradient ascent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ch2aMYsbmqt"
      },
      "source": [
        "# Finding the scaler of the dataset by using preprocessing.StandardScaler().fit()\n",
        "# If a column is standardized, mean value of the column is subtracted from each value and then the values are divided by the standard deviation of the column. The resulting columns have a standard deviation of 1 and a mean that is very close to zero. Thus, we end up having variables (columns) that have almost a normal distribution.\n",
        "# Using this scale to scale the X_train and X_test using .transform() - we only fit the training set and tranform the others\n",
        "# Fitting the entire dataset to the standard scaler object causes the model to learn about test set which we don't want\n",
        "# We have 30 different attributes and all the attributes have different scales\n",
        "# Making sure each attributes has the equal impact on data\n",
        "scaler = sklearn.preprocessing.StandardScaler().fit(X_train)\n",
        "X_train_t = scaler.transform(X_train)\n",
        "X_test_t = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mo-gXmjbmqu",
        "outputId": "8e1e841c-7e55-480f-b960-7d2a545f031b"
      },
      "source": [
        "# Printing the shape of the training set x_train and y_train \n",
        "print(\"\\nThe shape of the training set: \")\n",
        "print(\"X ------\", X_train.shape) \n",
        "print(\"Y ------\", y_train.shape) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The shape of the training set: \n",
            "X ------ (426, 30)\n",
            "Y ------ (426,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw1BjQYcbmqw"
      },
      "source": [
        "#### Adding a column of ones to the  matrices $X_{train}$ and  $X_{test}$\n",
        "After adding a column of ones $X_{train}=\\left[\\begin{matrix}\n",
        "1& x^{(1)}_1& x^{(1)}_2 &\\ldots& x^{(1)}_d\\\\\n",
        "1& x^{(2)}_1& x^{(2)}_2 &\\ldots& x^{(2)}_d\\\\\n",
        "\\vdots & \\vdots &\\vdots & & \\vdots \\\\\n",
        "1& x^{(N')}_1& x^{(N')}_2 &\\ldots& x^{(N')}_d\\\\\n",
        "\\end{matrix}\\right]$\n",
        "\n",
        "Similarly for $X_{test}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPDPRgBPbmqx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e0c7a43-105a-4b52-b277-27b361e1d1ce"
      },
      "source": [
        "# Appending a column of ones to x_train and X_test\n",
        "\n",
        "# Creating a column vector of ones by using np.ones and reshape\n",
        "training_vector = np.ones(shape= y_train.shape).reshape(-1,1)\n",
        "# Appending the vector to the beginning of x_train by using np.hstack\n",
        "X_train = np.hstack((training_vector, X_train))\n",
        "\n",
        "# Doing the same for the test data\n",
        "testing_vector = np.ones(shape=y_test.shape).reshape(-1,1)\n",
        "X_test = np.hstack((testing_vector, X_test))\n",
        "\n",
        "# We can check that everything worked correctly by:\n",
        "# Printing out the new dimensions\n",
        "print(\"\\nThe new dimensions of data: \")\n",
        "print(\"X train ------\", X_train.shape) \n",
        "print(\"X test ------\", X_test.shape) \n",
        "\n",
        "# Looking at the first two rows of X_train to check everything worked as expected\n",
        "print(X_train[0:2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The new dimensions of data: \n",
            "X train ------ (426, 31)\n",
            "X test ------ (143, 31)\n",
            "[[ 1.          1.54012613  0.91229211  1.52119391  1.47640466  0.33018764\n",
            "   0.52038897  1.21532979  1.37174601  0.64760934 -0.56103238  0.74179351\n",
            "   2.9376887   1.24596273  0.65483514  1.39973045  0.86583299  1.41899736\n",
            "   3.66818714  0.86422218  0.92486089  0.94239329  0.77558863  1.03472391\n",
            "   0.76015112 -0.3186253  -0.08183965  0.53114111  1.0337912  -0.52538349\n",
            "  -0.43921564]\n",
            " [ 1.         -0.69222545  1.19852137 -0.64252731 -0.70672851  1.93852663\n",
            "   0.96385359 -0.5480159  -0.09309424  1.16604331  1.59514339 -0.39510764\n",
            "   1.38098164 -0.35051099 -0.42654888  1.94641928  0.52937796 -0.43912221\n",
            "   0.95117524 -0.59840614  0.76750834 -0.68113375  1.06056527 -0.62970874\n",
            "  -0.69065425  1.94769028  0.45060929 -0.63657612  0.24657612 -0.1581473\n",
            "   0.87303092]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTrwgqmVbmqx"
      },
      "source": [
        "### Understanding the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLxWPws2bmqx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cc33e21-fb13-446e-bb19-0673a4e36803"
      },
      "source": [
        "# Description of the dataset\n",
        "print(\"\\n\", cancer.DESCR)\n",
        "# Printing the names of all the features\n",
        "print(\"The number of features is: \", X.shape[1])\n",
        "print(cancer.feature_names, \"\\n\")\n",
        "print(cancer.keys(), \"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " .. _breast_cancer_dataset:\n",
            "\n",
            "Breast cancer wisconsin (diagnostic) dataset\n",
            "--------------------------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 569\n",
            "\n",
            "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
            "\n",
            "    :Attribute Information:\n",
            "        - radius (mean of distances from center to points on the perimeter)\n",
            "        - texture (standard deviation of gray-scale values)\n",
            "        - perimeter\n",
            "        - area\n",
            "        - smoothness (local variation in radius lengths)\n",
            "        - compactness (perimeter^2 / area - 1.0)\n",
            "        - concavity (severity of concave portions of the contour)\n",
            "        - concave points (number of concave portions of the contour)\n",
            "        - symmetry \n",
            "        - fractal dimension (\"coastline approximation\" - 1)\n",
            "\n",
            "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
            "        largest values) of these features were computed for each image,\n",
            "        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
            "        13 is Radius SE, field 23 is Worst Radius.\n",
            "\n",
            "        - class:\n",
            "                - WDBC-Malignant\n",
            "                - WDBC-Benign\n",
            "\n",
            "    :Summary Statistics:\n",
            "\n",
            "    ===================================== ====== ======\n",
            "                                           Min    Max\n",
            "    ===================================== ====== ======\n",
            "    radius (mean):                        6.981  28.11\n",
            "    texture (mean):                       9.71   39.28\n",
            "    perimeter (mean):                     43.79  188.5\n",
            "    area (mean):                          143.5  2501.0\n",
            "    smoothness (mean):                    0.053  0.163\n",
            "    compactness (mean):                   0.019  0.345\n",
            "    concavity (mean):                     0.0    0.427\n",
            "    concave points (mean):                0.0    0.201\n",
            "    symmetry (mean):                      0.106  0.304\n",
            "    fractal dimension (mean):             0.05   0.097\n",
            "    radius (standard error):              0.112  2.873\n",
            "    texture (standard error):             0.36   4.885\n",
            "    perimeter (standard error):           0.757  21.98\n",
            "    area (standard error):                6.802  542.2\n",
            "    smoothness (standard error):          0.002  0.031\n",
            "    compactness (standard error):         0.002  0.135\n",
            "    concavity (standard error):           0.0    0.396\n",
            "    concave points (standard error):      0.0    0.053\n",
            "    symmetry (standard error):            0.008  0.079\n",
            "    fractal dimension (standard error):   0.001  0.03\n",
            "    radius (worst):                       7.93   36.04\n",
            "    texture (worst):                      12.02  49.54\n",
            "    perimeter (worst):                    50.41  251.2\n",
            "    area (worst):                         185.2  4254.0\n",
            "    smoothness (worst):                   0.071  0.223\n",
            "    compactness (worst):                  0.027  1.058\n",
            "    concavity (worst):                    0.0    1.252\n",
            "    concave points (worst):               0.0    0.291\n",
            "    symmetry (worst):                     0.156  0.664\n",
            "    fractal dimension (worst):            0.055  0.208\n",
            "    ===================================== ====== ======\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
            "\n",
            "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
            "\n",
            "    :Donor: Nick Street\n",
            "\n",
            "    :Date: November, 1995\n",
            "\n",
            "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
            "https://goo.gl/U2Uwz2\n",
            "\n",
            "Features are computed from a digitized image of a fine needle\n",
            "aspirate (FNA) of a breast mass.  They describe\n",
            "characteristics of the cell nuclei present in the image.\n",
            "\n",
            "Separating plane described above was obtained using\n",
            "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
            "Construction Via Linear Programming.\" Proceedings of the 4th\n",
            "Midwest Artificial Intelligence and Cognitive Science Society,\n",
            "pp. 97-101, 1992], a classification method which uses linear\n",
            "programming to construct a decision tree.  Relevant features\n",
            "were selected using an exhaustive search in the space of 1-4\n",
            "features and 1-3 separating planes.\n",
            "\n",
            "The actual linear program used to obtain the separating plane\n",
            "in the 3-dimensional space is that described in:\n",
            "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
            "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
            "Optimization Methods and Software 1, 1992, 23-34].\n",
            "\n",
            "This database is also available through the UW CS ftp server:\n",
            "\n",
            "ftp ftp.cs.wisc.edu\n",
            "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
            "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
            "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
            "     San Jose, CA, 1993.\n",
            "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
            "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
            "     July-August 1995.\n",
            "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
            "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
            "     163-171.\n",
            "The number of features is:  30\n",
            "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
            " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
            " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
            " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
            " 'smoothness error' 'compactness error' 'concavity error'\n",
            " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
            " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
            " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
            " 'worst concave points' 'worst symmetry' 'worst fractal dimension'] \n",
            "\n",
            "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename']) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX3LmmUybmqy"
      },
      "source": [
        "\n",
        " \n",
        "### Sigmoid($z$)\n",
        "\n",
        "sigmoid($z$) takes as input a column vector of real numbers, $z^T = [z_1, z_2, ..., z_{N'}]$, where $N'$ is the number of  examples\n",
        "\n",
        "It should produce as output a column vector $\\left[\\frac{1}{1+e^{-z_1}},\\frac{1}{1+e^{-z_2}},...,\\frac{1}{1+e^{-z_{N'}}}\\right]^T$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s7wkVrhbmqz"
      },
      "source": [
        "# Writing the sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfGAitIAbmqz"
      },
      "source": [
        "### Initializing ${\\bf w}$\n",
        "For testing the next functions, we create a coefficient vector, ${\\bf w}$.\n",
        "We will initialize the coeffients to be $0$, i.e. ${\\bf w}^T = [0,0,\\ldots ,0]$ (We could have initialized ${\\bf w}$ to any values.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqKJkSJHbmqz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25735012-b1ed-4775-f406-aa46190b2c95"
      },
      "source": [
        "# Initializing w using np.zeros()\n",
        "w = np.zeros((X_train.shape[1])) \n",
        "print(\"The shape of w: \", w.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of w:  (31,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7vJzu4abmq0"
      },
      "source": [
        "### Our hypothesis, $h({\\bf x})$\n",
        "The next  function to write is our hypothesis function. \n",
        "\n",
        "For example if our design matrix $X$ consists of single example $X=[1,x_1,x_2,\\ldots,x_d]$ and  weights ${\\bf w}^T=[w_0,w_2,\\ldots, w_d]$, it returns $h({\\bf x})=\\frac{1}{1+e^{-\\left({w_{0}\\cdot 1 +w_1\\cdot x_1+\\cdots w_d\\cdot x_d}\\right)}}$\n",
        "\n",
        "If given a  matrix consisting of $N'$ examples \n",
        "$X=\\left[\\begin{matrix}\n",
        "1& x^{(1)}_1& x^{(1)}_2 &\\ldots& x^{(1)}_d\\\\\n",
        "1& x^{(2)}_1& x^{(2)}_2 &\\ldots& x^{(2)}_d\\\\\n",
        "\\vdots & \\vdots &\\vdots & & \\vdots \\\\\n",
        "1& x^{(N')}_1& x^{(N')}_2 &\\ldots& x^{(N')}_d\\\\\n",
        "\\end{matrix}\\right]$\n",
        "and  weights ${\\bf w}^T=[w_0,w_2,\\ldots, w_d]$, the function returns a column vector\n",
        "$[h({\\bf x}^{(1)}),h({\\bf x}^{(2)},\\ldots, h({\\bf x}^{(N')}]^T$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPGifFvtbmq0"
      },
      "source": [
        "# Predicting the probability that a patient has cancer \n",
        "# Writing the hypothesis function \n",
        "def hypothesis(X , w):\n",
        "    return sigmoid(np.dot(X, w))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvS7P6dabmq0"
      },
      "source": [
        "### Log-Likelihood Function.\n",
        "Calculating the log likelihood function $\\ell({\\bf w})=\n",
        "\\sum_{i=1}^{N'}y^{(i)}\\ln(h({\\bf x}^{(i)})) +(1- y^{(i)})\\ln(1-h({\\bf x}^{(i)}))$\n",
        "\n",
        "The input is a matrix consisting of $N'$ examples $X=\\left[\\begin{matrix}\n",
        "1& x^{(1)}_1& x^{(1)}_2 &\\ldots& x^{(1)}_d\\\\\n",
        "1& x^{(2)}_1& x^{(2)}_2 &\\ldots& x^{(2)}_d\\\\\n",
        "\\vdots & \\vdots &\\vdots & & \\vdots \\\\\n",
        "1& x^{(N')}_1& x^{(N')}_2 &\\ldots& x^{(N')}_d\\\\\n",
        "\\end{matrix}\\right]$\n",
        "and a column vector ${\\bf y}^T=[y^{(1)},y^{(2)},\\dots,y^{(N')}]$ of labels for $X$.\n",
        "\n",
        "The output is $\\ell({\\bf w})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ykFVCWQbmq1"
      },
      "source": [
        "# Writing the log likelihood function \n",
        "def log_likelihood(X , y , w ):\n",
        "    val = np.sum(y * np.log(hypothesis(X, w)) + (1 - y) * np.log(1 - hypothesis(X, w)))\n",
        "    return val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhVY_O4Jbmq1"
      },
      "source": [
        "# Gradient Ascent\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgZ6klb5bmq1"
      },
      "source": [
        "def Logistic_Regresion_Gradient_Ascent(X, y, learning_rate, num_iters):\n",
        "    # Initializing log_likelihood to be an empty list\n",
        "    log_likelihood_values = []  \n",
        "    # Initializing w to be a zero vector of shape x_train.shape[1],1\n",
        "    w = np.zeros((X_train.shape[1],1))\n",
        "    # Initialize N to the number of training examples\n",
        "    N = X.shape[0]\n",
        "    # For every 100 iterations, storing the log_likelihood for the current w\n",
        "    for i in range(num_iters):\n",
        "        # Calculating the hypothesis and derivative\n",
        "        h = hypothesis(X, w)\n",
        "        derivative = 1/N * np.matmul(np.transpose(X), (y - h))\n",
        "        # Calculating the update\n",
        "        update = learning_rate * derivative\n",
        "        # Updating w \n",
        "        w = w + update\n",
        "        # Appending the log_likelihodd values to the list for every 100 iterations\n",
        "        if (i % 100 == 0):\n",
        "            log_likelihood_values.append(log_likelihood(X, y, w))\n",
        "            \n",
        "    return w, log_likelihood_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDx7jP4ebmq2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ae67416-5da0-46eb-9211-ca6142b24ad3"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "# Reshaping Y train\n",
        "y_train = y_train.reshape(y_train.shape[0],1)\n",
        "# Setting the learning_rate\n",
        "learning_rate = 0.5\n",
        "# Setting the num_iters\n",
        "num_iters = 5000\n",
        "# Running the Logistic_Regresion_Gradient_Ascent() and storing the returned values\n",
        "w, log_likelihood_values = Logistic_Regresion_Gradient_Ascent(X_train, y_train, learning_rate, num_iters)\n",
        "print(w)\n",
        "print(log_likelihood_values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(426, 31)\n",
            "(426,)\n",
            "[[-0.97616379]\n",
            " [ 0.71728242]\n",
            " [ 0.17113221]\n",
            " [ 0.66254775]\n",
            " [ 0.1886061 ]\n",
            " [ 0.37175963]\n",
            " [ 2.12568488]\n",
            " [-2.30252242]\n",
            " [-2.11423871]\n",
            " [ 0.5618252 ]\n",
            " [-0.30298111]\n",
            " [-3.09861638]\n",
            " [ 0.16118323]\n",
            " [-1.77209556]\n",
            " [-2.64756391]\n",
            " [ 1.11770733]\n",
            " [ 0.83515656]\n",
            " [ 0.88529849]\n",
            " [ 0.18250726]\n",
            " [-0.56334676]\n",
            " [ 1.77643413]\n",
            " [-2.2138097 ]\n",
            " [-2.26723799]\n",
            " [-1.66351709]\n",
            " [-2.45124575]\n",
            " [-1.70484877]\n",
            " [ 0.83783358]\n",
            " [-1.87952408]\n",
            " [-1.8441974 ]\n",
            " [-0.69199867]\n",
            " [-1.66154804]]\n",
            "[-102.60193784287699, -32.018923900701495, -28.48877971321679, -26.79720527020011, -25.715554411722096, -24.926798427385584, -24.30886855270296, -23.803235049780533, -23.377137492865472, -23.010057044190752, -22.68818987684395, -22.401872511970023, -22.144166667094918, -21.90997463921455, -21.69545924478112, -21.49766167037383, -21.314249310982156, -21.1433473716694, -20.983424010702556, -20.833210002039067, -20.691641100588917, -20.557815715356238, -20.43096317211197, -20.310419480133508, -20.195608533158705, -20.086027321870652, -19.981234158153278, -19.880839194513975, -19.784496715925982, -19.691898816763974, -19.60277017182502, -19.516863680044082, -19.43395681055906, -19.353848518695894, -19.276356627937687, -19.20131559558577, -19.12857459642255, -19.057995871538193, -18.989453299516974, -18.92283115507535, -18.858023026504803, -18.794930868271976, -18.733464169147624, -18.67353921948728, -18.61507846393278, -18.558009927969298, -18.50226670855494, -18.44778652051366, -18.394511291607085, -18.34238680022344]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymnp-ozqbmq3"
      },
      "source": [
        "# Plotting Likelihood v/s Number of Iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHFvGBUybmq3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "b5359272-f539-429e-b7a2-a4bf19b75137"
      },
      "source": [
        "# Running this cell to plot Likelihood v/s Number of Iterations.\n",
        "iters = np.array(range(0,num_iters,100))\n",
        "plt.plot(iters,log_likelihood_values,'.-',color='violet')\n",
        "plt.xlabel('Number of iterations')\n",
        "plt.ylabel('Likelihood')\n",
        "plt.title(\"Likelihood vs Number of Iterations.\")\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f51a4f3ceb8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Number of iterations')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Likelihood')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Likelihood vs Number of Iterations.')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCjhigs0bmq3"
      },
      "source": [
        "#  Evaluating your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqzw2mEvbmq4"
      },
      "source": [
        "# Using hypothesis(...) to predict.\n",
        "threshold = 0.5\n",
        "prediction_1 = hypothesis(X_test,w)\n",
        "prediction_2 = prediction_1.reshape((prediction_1.shape[0]))\n",
        "after_threshold = (prediction_2 >= threshold).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2EyRXribmq4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2da2655a-20ac-405e-991f-1125e811e057"
      },
      "source": [
        "# Summarizing the efficiency of the program\n",
        "def confusion_matrix(y_target, y_predicted):\n",
        "  TP=0\n",
        "  FP=0\n",
        "  FN=0\n",
        "  TN=0\n",
        "\n",
        "  for i in range(y_test.shape[0]):\n",
        "    # Predicting positive and it's true\n",
        "    if ((y_target[i]) == 1 and (y_predicted[i] == 1)):\n",
        "      TP += 1\n",
        "    # Predicting positive and it's false\n",
        "    elif ((y_target[i]) == 0 and (y_predicted[i] == 1)):\n",
        "      FP += 1\n",
        "    # Predicting negative and it's false \n",
        "    elif ((y_target[i]) == 1 and (y_predicted[i] == 0)):\n",
        "      FN += 1\n",
        "    # Predicting negative and it's true\n",
        "    elif ((y_target[i]) == 0 and (y_predicted[i] == 0)):\n",
        "      TN += 1\n",
        "\n",
        "  # Out of all the positive classes we have predicted correctly, how many are actually positive\n",
        "  precision = TP/(TP+FP)\n",
        "  # Out of all the positive classes, how much we predicted correctly. It should be high as possible.\n",
        "  recall = TP/(TP+FN)\n",
        "  f1 = 2*(precision*recall)/(precision+recall)\n",
        "\n",
        "  print(\"Precision: \",precision)\n",
        "  print(\"Recall: \",recall)\n",
        "  print(\"F1: \",f1)\n",
        "  print(\"Confusion Matrix: \")\n",
        "  print(\"TP: \",TP,\" FN: \",FN,\" FP: \",FP,\" TN: \",TN)\n",
        "\n",
        "\n",
        "confusion_matrix(y_test, after_threshold)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision:  0.968421052631579\n",
            "Recall:  0.989247311827957\n",
            "F1:  0.9787234042553192\n",
            "Confusion Matrix: \n",
            "TP:  92  FN:  1  FP:  3  TN:  47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S91Luaec4vgy",
        "outputId": "49a4d3d4-80a5-4e58-c96a-2de7a2c36afe"
      },
      "source": [
        "# Checking out different hyper-parameters for better Precision, Recall and F1 manually\n",
        "threshold = 0.38\n",
        "prediction_1 = hypothesis(X_test,w)\n",
        "prediction_2 = prediction_1.reshape((prediction_1.shape[0]))\n",
        "after_threshold = (prediction_2 >= threshold).astype(int)\n",
        "# Setting the learning_rate\n",
        "learning_rate = 0.8\n",
        "# Setting the num_iters\n",
        "num_iters = 100\n",
        "# Running the Logistic_Regresion_Gradient_Ascent() and storing the returned values\n",
        "w, log_likelihood_values = Logistic_Regresion_Gradient_Ascent(X_train, y_train, learning_rate, num_iters)\n",
        "confusion_matrix(y_test, after_threshold)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision:  0.9587628865979382\n",
            "Recall:  1.0\n",
            "F1:  0.9789473684210526\n",
            "Confusion Matrix: \n",
            "TP:  93  FN:  0  FP:  4  TN:  46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-o09jej9n5z"
      },
      "source": [
        "These are some better values for hyper-parameters found since recall is 1.0 and it should be as high as possible since it indicates how many predictions we made correctly. Precision also increased a bit."
      ]
    }
  ]
}